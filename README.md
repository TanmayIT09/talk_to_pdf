# ğŸ“„ Talk to PDF â€“ Local Chatbot using Ollama + LangChain + Streamlit

A local, open-source **PDF chatbot** that lets you upload a PDF and chat with it using an LLM.
No paid APIs, no quotas â€” everything runs on your own machine.

---

## âœ¨ Features

* Chat-style UI (ChatGPT-like)
* Context retained **per PDF**
* Automatically resets context when a new PDF is uploaded
* Local vector database using **Chroma**
* Open-source LLM & embeddings via **Ollama**
* Optimized for fast Streamlit reruns

---

## ğŸ§  Tech Stack

* **Streamlit** â€“ UI
* **LangChain** â€“ orchestration
* **Ollama** â€“ local LLM runtime
* **Llama 3.1** â€“ chat model
* **nomic-embed-text** â€“ embeddings
* **ChromaDB** â€“ vector store
* **pdfplumber** â€“ PDF text extraction

---

## ğŸ“‹ Prerequisites

### 1ï¸âƒ£ Python

* Python **3.9 â€“ 3.11** recommended
  Check version:

```bash
python --version
```

---

### 2ï¸âƒ£ Ollama (Required)

Ollama runs the LLM **locally**.

#### Install Ollama

ğŸ‘‰ [https://ollama.com/download](https://ollama.com/download)

Verify installation:

```bash
ollama --version
```

---

### 3ï¸âƒ£ Pull Required Models

Run these **once**:

```bash
ollama pull llama3.1
ollama pull nomic-embed-text
```

You can test Ollama:

```bash
ollama run llama3.1
```

---

## ğŸš€ Project Setup

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/<your-username>/talk_to_pdf.git
cd talk_to_pdf
```

---

### 2ï¸âƒ£ Create Virtual Environment (Recommended)

#### Windows

```bash
python -m venv venv
venv\\Scripts\\activate
```

#### macOS / Linux

```bash
python3 -m venv venv
source venv/bin/activate
```

---

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

If `requirements.txt` doesnâ€™t exist yet, create one with:

```txt
streamlit
pdfplumber
langchain
langchain-community
chromadb
ollama
```

---

## â–¶ï¸ Run the Application

```bash
streamlit run talk_to_pdf_app.py
```

The app will open in your browser at:

```
http://localhost:8501
```

---

## ğŸ§ª How to Use

1. Upload a PDF
2. Wait for vector indexing (first time only)
3. Ask questions in the chat input
4. Continue chatting â€” context is retained
5. Upload a new PDF â†’ context automatically resets

---

## ğŸ“‚ Project Structure

```
talk_to_pdf/
â”‚
â”œâ”€â”€ talk_to_pdf_app.py     # Main Streamlit app (single file)
â”œâ”€â”€ chroma_db/             # Vector DB (auto-created)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

---

## âš ï¸ Important Notes

### ğŸ”¹ Streamlit Cloud

* Ollama **does NOT run on Streamlit Cloud**
* This project is meant for:

  * Local laptop usage
  * Self-hosted VM / server

If you want a **cloud-deployable version**, youâ€™ll need:

* Gemini / OpenAI / Groq instead of Ollama

(I can help you convert it.)

---

## ğŸ§¹ Cleanup (Optional)

To reset all stored embeddings:

```bash
rm -rf chroma_db
```

---

## ğŸ› ï¸ Common Issues

### âŒ `Ollama not found`

* Ensure Ollama is installed
* Restart terminal after installation

### âŒ Slow first response

* First query loads model into memory
* Subsequent queries are much faster

---

## ğŸ“Œ Roadmap Ideas

* Conversation summarization instead of trimming
* Multi-PDF support
* Source citations in answers
* Hybrid local + cloud LLM mode

---

## ğŸ‘¤ Author

Built by **Tanmay Srivastava**
Feel free to fork, improve, and share ğŸš€
